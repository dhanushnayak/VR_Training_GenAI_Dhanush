{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for Machine Learning\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand calculus concepts: derivatives, gradients, partial derivatives\n",
    "- Master linear algebra: vectors, matrices, operations\n",
    "- Learn probability and statistics fundamentals\n",
    "- Explore optimization techniques: gradient descent, cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus for ML\n",
    "\n",
    "### Derivatives\n",
    "- **Definition**: Rate of change of a function\n",
    "- **ML Application**: Finding optimal parameters by minimizing cost functions\n",
    "- **Chain Rule**: Essential for backpropagation in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import sympy as sp\n",
    "\n",
    "# Derivative example\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x**2 + 2*x + 1  # f(x) = x² + 2x + 1\n",
    "dy_dx = 2*x + 2     # f'(x) = 2x + 2\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original function\n",
    "ax1.plot(x, y, 'b-', label='f(x) = x² + 2x + 1')\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.set_title('Original Function')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('f(x)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative\n",
    "ax2.plot(x, dy_dx, 'r-', label=\"f'(x) = 2x + 2\")\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=-1, color='g', linestyle=':', label='Minimum at x=-1')\n",
    "ax2.set_title('Derivative (Slope)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel(\"f'(x)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Derivative = 0 at minimum (x = -1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives\n",
    "- **Definition**: Derivative with respect to one variable, holding others constant\n",
    "- **ML Application**: Gradients in multivariable optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial derivatives example\n",
    "# f(x,y) = x² + y² + 2xy\n",
    "x = np.linspace(-2, 2, 50)\n",
    "y = np.linspace(-2, 2, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2 + 2*X*Y\n",
    "\n",
    "# Partial derivatives\n",
    "# ∂f/∂x = 2x + 2y\n",
    "# ∂f/∂y = 2y + 2x\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, alpha=0.7, cmap='viridis')\n",
    "ax1.set_title('f(x,y) = x² + y² + 2xy')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_title('Contour Plot')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient field\n",
    "ax3 = fig.add_subplot(133)\n",
    "dx = 2*X + 2*Y  # ∂f/∂x\n",
    "dy = 2*Y + 2*X  # ∂f/∂y\n",
    "ax3.quiver(X[::5, ::5], Y[::5, ::5], dx[::5, ::5], dy[::5, ::5], alpha=0.7)\n",
    "ax3.contour(X, Y, Z, levels=10, alpha=0.3)\n",
    "ax3.set_title('Gradient Field')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient points in direction of steepest ascent\")\n",
    "print(\"Negative gradient points toward minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra for ML\n",
    "\n",
    "### Vectors and Matrices\n",
    "- **Vectors**: Represent data points, features, parameters\n",
    "- **Matrices**: Represent datasets, transformations, weights\n",
    "- **Operations**: Dot products, matrix multiplication, eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector operations\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(\"Vector Operations:\")\n",
    "print(f\"v1 = {v1}\")\n",
    "print(f\"v2 = {v2}\")\n",
    "print(f\"v1 + v2 = {v1 + v2}\")\n",
    "print(f\"Dot product: v1 · v2 = {np.dot(v1, v2)}\")\n",
    "print(f\"Magnitude of v1: ||v1|| = {np.linalg.norm(v1):.3f}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"\\nMatrix Operations:\")\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix B:\\n{B}\")\n",
    "print(f\"A + B:\\n{A + B}\")\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"A^T (transpose):\\n{A.T}\")\n",
    "print(f\"det(A) = {np.linalg.det(A)}\")\n",
    "print(f\"A^(-1) (inverse):\\n{np.linalg.inv(A)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML application: Linear regression using matrix operations\n",
    "# y = Xβ + ε\n",
    "# Solution: β = (X^T X)^(-1) X^T y\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_data = np.random.randn(n_samples, 1)\n",
    "y_data = 2 * X_data.ravel() + 1 + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "# Add bias term (intercept)\n",
    "X_matrix = np.column_stack([np.ones(n_samples), X_data])\n",
    "\n",
    "# Analytical solution using linear algebra\n",
    "beta = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_data\n",
    "\n",
    "print(f\"True parameters: intercept=1, slope=2\")\n",
    "print(f\"Estimated parameters: intercept={beta[0]:.3f}, slope={beta[1]:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_data, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(X_data.min(), X_data.max(), 100)\n",
    "y_line = beta[0] + beta[1] * x_line\n",
    "plt.plot(x_line, y_line, 'r-', label=f'Fitted line: y = {beta[0]:.2f} + {beta[1]:.2f}x')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression using Matrix Operations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability and Statistics\n",
    "\n",
    "### Key Concepts\n",
    "- **Probability Distributions**: Normal, Bernoulli, Poisson\n",
    "- **Expectation and Variance**: Central tendencies and spread\n",
    "- **Bayes' Theorem**: Foundation for probabilistic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Common probability distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Normal distribution\n",
    "x_norm = np.linspace(-4, 4, 100)\n",
    "y_norm = stats.norm.pdf(x_norm, 0, 1)\n",
    "axes[0, 0].plot(x_norm, y_norm, 'b-', label='μ=0, σ=1')\n",
    "axes[0, 0].fill_between(x_norm, y_norm, alpha=0.3)\n",
    "axes[0, 0].set_title('Normal Distribution')\n",
    "axes[0, 0].set_xlabel('x')\n",
    "axes[0, 0].set_ylabel('Probability Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bernoulli distribution\n",
    "x_bern = [0, 1]\n",
    "p = 0.3\n",
    "y_bern = [1-p, p]\n",
    "axes[0, 1].bar(x_bern, y_bern, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title(f'Bernoulli Distribution (p={p})')\n",
    "axes[0, 1].set_xlabel('Outcome')\n",
    "axes[0, 1].set_ylabel('Probability')\n",
    "axes[0, 1].set_xticks([0, 1])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Poisson distribution\n",
    "x_pois = np.arange(0, 15)\n",
    "lambda_param = 3\n",
    "y_pois = stats.poisson.pmf(x_pois, lambda_param)\n",
    "axes[1, 0].bar(x_pois, y_pois, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title(f'Poisson Distribution (λ={lambda_param})')\n",
    "axes[1, 0].set_xlabel('Number of Events')\n",
    "axes[1, 0].set_ylabel('Probability')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Central Limit Theorem demonstration\n",
    "sample_means = []\n",
    "for _ in range(1000):\n",
    "    sample = np.random.exponential(2, 30)  # Non-normal distribution\n",
    "    sample_means.append(np.mean(sample))\n",
    "\n",
    "axes[1, 1].hist(sample_means, bins=30, alpha=0.7, density=True, color='purple')\n",
    "axes[1, 1].set_title('Central Limit Theorem\\n(Sample means are normal)')\n",
    "axes[1, 1].set_xlabel('Sample Mean')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample means: μ = {np.mean(sample_means):.3f}, σ = {np.std(sample_means):.3f}\")\n",
    "print(f\"Theoretical: μ = 2.0, σ = {2/np.sqrt(30):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "**P(A|B) = P(B|A) × P(A) / P(B)**\n",
    "\n",
    "- **P(A|B)**: Posterior probability\n",
    "- **P(B|A)**: Likelihood\n",
    "- **P(A)**: Prior probability\n",
    "- **P(B)**: Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' Theorem example: Medical diagnosis\n",
    "# Disease prevalence: 1%\n",
    "# Test accuracy: 95% (both sensitivity and specificity)\n",
    "\n",
    "P_disease = 0.01  # Prior: 1% of population has disease\n",
    "P_no_disease = 0.99\n",
    "\n",
    "P_positive_given_disease = 0.95  # Sensitivity\n",
    "P_negative_given_no_disease = 0.95  # Specificity\n",
    "P_positive_given_no_disease = 0.05  # False positive rate\n",
    "\n",
    "# Total probability of positive test\n",
    "P_positive = (P_positive_given_disease * P_disease + \n",
    "              P_positive_given_no_disease * P_no_disease)\n",
    "\n",
    "# Bayes' theorem: P(disease|positive test)\n",
    "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(\"Bayes' Theorem Example: Medical Diagnosis\")\n",
    "print(f\"Prior probability of disease: {P_disease:.1%}\")\n",
    "print(f\"Test accuracy: {P_positive_given_disease:.1%}\")\n",
    "print(f\"Probability of positive test: {P_positive:.3f}\")\n",
    "print(f\"Probability of disease given positive test: {P_disease_given_positive:.1%}\")\n",
    "print(\"\\nKey insight: Even with 95% accurate test, only 16% chance of having disease!\")\n",
    "\n",
    "# Visualization\n",
    "categories = ['Prior\\n(Disease)', 'Posterior\\n(Disease|Positive)']\n",
    "probabilities = [P_disease, P_disease_given_positive]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(categories, probabilities, color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Bayes\\' Theorem: Updating Beliefs with Evidence')\n",
    "plt.ylim(0, 0.2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, prob in zip(bars, probabilities):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Gradient Descent\n",
    "\n",
    "### Cost Functions\n",
    "- **Purpose**: Measure how well model fits data\n",
    "- **Examples**: Mean Squared Error, Cross-entropy\n",
    "- **Goal**: Minimize cost function to find optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent implementation\n",
    "def gradient_descent_1d(f, df, x_start, learning_rate=0.1, max_iter=100):\n",
    "    \"\"\"1D gradient descent\"\"\"\n",
    "    x = x_start\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        gradient = df(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        history.append(x)\n",
    "        \n",
    "        if abs(gradient) < 1e-6:  # Convergence check\n",
    "            break\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Example function: f(x) = x² - 4x + 4 = (x-2)²\n",
    "def f(x):\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "def df(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "# Run gradient descent\n",
    "x_optimal, history = gradient_descent_1d(f, df, x_start=0, learning_rate=0.3)\n",
    "\n",
    "print(f\"Optimal x: {x_optimal:.6f}\")\n",
    "print(f\"Minimum value: {f(x_optimal):.6f}\")\n",
    "print(f\"Iterations: {len(history)-1}\")\n",
    "\n",
    "# Visualization\n",
    "x_plot = np.linspace(-1, 5, 100)\n",
    "y_plot = f(x_plot)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Function and optimization path\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_plot, 'b-', label='f(x) = (x-2)²')\n",
    "plt.plot(history, [f(x) for x in history], 'ro-', alpha=0.7, label='Gradient descent path')\n",
    "plt.plot(2, 0, 'g*', markersize=15, label='True minimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Optimization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(history)), history, 'o-')\n",
    "plt.axhline(y=2, color='g', linestyle='--', label='True minimum')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x value')\n",
    "plt.title('Convergence to Minimum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D gradient descent for linear regression\n",
    "def gradient_descent_linear_regression(X, y, learning_rate=0.01, max_iter=1000):\n",
    "    \"\"\"Gradient descent for linear regression\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # Initialize parameters\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Predictions\n",
    "        h = X @ theta\n",
    "        \n",
    "        # Cost (MSE)\n",
    "        cost = (1/(2*m)) * np.sum((h - y)**2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Gradients\n",
    "        gradients = (1/m) * X.T @ (h - y)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        # Check convergence\n",
    "        if i > 0 and abs(cost_history[-2] - cost_history[-1]) < 1e-8:\n",
    "            break\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X_raw = np.random.randn(m, 1)\n",
    "y = 4 + 3 * X_raw.ravel() + np.random.randn(m)\n",
    "X = np.column_stack([np.ones(m), X_raw])  # Add bias term\n",
    "\n",
    "# Run gradient descent\n",
    "theta_gd, cost_history = gradient_descent_linear_regression(X, y, learning_rate=0.01)\n",
    "\n",
    "print(f\"True parameters: intercept=4, slope=3\")\n",
    "print(f\"Gradient descent: intercept={theta_gd[0]:.3f}, slope={theta_gd[1]:.3f}\")\n",
    "\n",
    "# Compare with analytical solution\n",
    "theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f\"Analytical solution: intercept={theta_analytical[0]:.3f}, slope={theta_analytical[1]:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Data and fitted line\n",
    "ax1.scatter(X_raw, y, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(X_raw.min(), X_raw.max(), 100)\n",
    "y_line_gd = theta_gd[0] + theta_gd[1] * x_line\n",
    "y_line_analytical = theta_analytical[0] + theta_analytical[1] * x_line\n",
    "ax1.plot(x_line, y_line_gd, 'r-', label='Gradient Descent')\n",
    "ax1.plot(x_line, y_line_analytical, 'g--', label='Analytical Solution')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Linear Regression: Gradient Descent vs Analytical')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost function convergence\n",
    "ax2.plot(cost_history)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Cost (MSE)')\n",
    "ax2.set_title('Cost Function Convergence')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
    "print(f\"Iterations to convergence: {len(cost_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Calculus\n",
    "- **Derivatives**: Find optimal points by setting gradient to zero\n",
    "- **Partial derivatives**: Handle multivariable functions\n",
    "- **Chain rule**: Essential for neural network backpropagation\n",
    "\n",
    "### Linear Algebra\n",
    "- **Vectors and matrices**: Represent data and transformations\n",
    "- **Matrix operations**: Efficient computation for ML algorithms\n",
    "- **Eigenvalues/eigenvectors**: PCA, dimensionality reduction\n",
    "\n",
    "### Probability and Statistics\n",
    "- **Distributions**: Model uncertainty and variability\n",
    "- **Bayes' theorem**: Update beliefs with new evidence\n",
    "- **Central limit theorem**: Foundation for statistical inference\n",
    "\n",
    "### Optimization\n",
    "- **Gradient descent**: Iterative optimization algorithm\n",
    "- **Cost functions**: Objective functions to minimize\n",
    "- **Learning rate**: Controls convergence speed and stability\n",
    "\n",
    "## Next Steps\n",
    "- Apply these concepts to deep learning frameworks\n",
    "- Explore advanced optimization techniques (Adam, RMSprop)\n",
    "- Learn about regularization and constraint optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}